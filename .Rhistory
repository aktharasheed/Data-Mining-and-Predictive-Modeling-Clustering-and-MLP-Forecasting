knitr::opts_chunk$set(echo = TRUE)
# Install necessary packages if not already installed
install.packages(c("NbClust", "cluster", "readxl", "dplyr", "ggplot2", "clusterCrit"))
# Load necessary libraries
library(NbClust)
library(cluster)
library(readxl)
library(dplyr)
library(ggplot2)
library(clusterCrit)
# Loading the dataset
file_path <- "vehicles.xlsx"
vehicles <- read_excel(file_path)
# Displaying the first few rows of the dataset
head(vehicles)
# Extracting the first 18 attributes for clustering
data <- vehicles %>% select(-Class)
# Defining a function to identify and replace outliers with NA by using the Interquartile Range (IQR) method
remove_outliers <- function(x) {
lowerq <- quantile(x, probs = 0.25, na.rm = TRUE)
upperq <- quantile(x, probs = 0.75, na.rm = TRUE)
iqr <- IQR(x, na.rm = TRUE)
x_cleaned <- ifelse(x < lowerq - 1.5 * iqr | x > upperq + 1.5 * iqr, NA, x)
return(x_cleaned)
}
# Creating a boxplot to visualize the data distribution before outlier removal
boxplot(data, main = "Box plot before Outlier Removal", col = "#03ff00")
# Applying the outlier removal function to each column of the dataset
data_no_outliers <- as.data.frame(lapply(data, remove_outliers))
# Creating a boxplot to visualize the data distribution after outlier removal
boxplot(data_no_outliers, main = "Box plot after Outlier Removal", col = "#FF0000")
# Removing rows containing NAs (outliers were replaced with NAs)
data_cleaned <- na.omit(data_no_outliers)
# Scaling the dataset and normalizing the features
scaled_data <- scale(data_cleaned)
# Creating a boxplot to visualize the data distribution after scaling
boxplot(scaled_data, main = "Box plot after Scaling", col = "#0000FF")
nbclust_results_euclidean <- NbClust(scaled_data, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
best_cluster_counts <- table(nbclust_results_euclidean$Best.nc[1, ])
barplot(best_cluster_counts, main="Optimal number of clusters - k based on 'Euclidean distance'",
xlab="Number of clusters k", ylab="Frequency among all indices", col="#0073C2FF")
# Elbow method
wss <- (nrow(scaled_data) - 1) * sum(apply(scaled_data, 2, var))
for (i in 2:10) wss[i] <- sum(kmeans(scaled_data, centers = i, nstart = 25)$tot.withinss)
plot(1:10, wss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of clusters K", ylab = "Total within-clusters sum of squares",
main = "Elbow Method for Optimal Clusters")
abline(v = which.min(diff(diff(wss))), lty = 2)
# Gap statistics
set.seed(123)
gap_stat <- clusGap(scaled_data, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
plot(gap_stat, main = "Gap Statistics for Optimal Clusters")
k_optimal_gap <- maxSE(gap_stat$Tab[, "gap"], gap_stat$Tab[, "SE.sim"])
# Silhouette analysis
sil_width <- numeric(10)
for (k in 2:10) {
km.res <- kmeans(scaled_data, centers = k, nstart = 25)
ss <- silhouette(km.res$cluster, dist(scaled_data))
sil_width[k] <- mean(ss[, 3])
}
plot(1:10, sil_width, type = "b", pch = 19, frame = FALSE,
xlab = "Number of clusters K", ylab = "Average silhouette width",
main = "Silhouette Method for Optimal Clusters")
k_optimal_sil <- which.max(sil_width)
# Determine the optimal k using majority vote
k_optimal <- as.numeric(names(sort(table(c(nbclust_results_euclidean$Best.nc[1,], k_optimal_gap, k_optimal_sil)), decreasing = TRUE)[1]))
set.seed(123)
kmeans_result <- kmeans(scaled_data, centers = k_optimal, nstart = 25)
# Display the clustering result
print(kmeans_result)
# K-means clustering result plot
clusplot(scaled_data, kmeans_result$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
# BSS/TSS ratio
bss <- kmeans_result$betweenss
tss <- kmeans_result$totss
bss_tss_ratio <- bss / tss
print(bss_tss_ratio)
# Silhouette plot
sil <- silhouette(kmeans_result$cluster, dist(scaled_data))
plot(sil, border = NA)
avg_sil_width <- mean(sil[, 'sil_width'])
print(avg_sil_width)
# Covariance matrix
cov_matrix <- cov(scaled_data)
print(cov_matrix)
# Performing PCA
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
summary(pca_result)
# Determining the number of components that explain at least 92% of the variance
cum_var_exp <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
num_components <- which(cum_var_exp >= 0.92)[1]
# Creating the PCA-transformed dataset
pca_data <- as.data.frame(pca_result$x[, 1:num_components])
# Ensure NbClust package is installed and loaded
if (!require("NbClust")) {
install.packages("NbClust")
}
library(NbClust)
# Debugging steps to ensure pca_data is correctly prepared
print(head(pca_data))
print(str(pca_data))
# Verify that there are no missing values in pca_data
if (any(is.na(pca_data))) {
stop("pca_data contains missing values. Please handle missing values before clustering.")
}
# NbClust analysis
tryCatch({
nbclust_results_euclidean_pca <- NbClust(pca_data, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
best_cluster_counts_pca <- table(nbclust_results_euclidean_pca$Best.nc[1, ])
barplot(best_cluster_counts_pca, main="after PCA Optimal number of clusters - k based on 'Euclidean distance'",
xlab="Number of clusters k", ylab="Frequency among all indices", col="#0073C2FF")
}, error = function(e) {
cat("Error in NbClust function: ", e$message, "\n")
})
# Elbow method after PCA
wss_pca <- (nrow(pca_data) - 1) * sum(apply(pca_data, 2, var))
for (i in 2:10) wss_pca[i] <- sum(kmeans(pca_data, centers = i, nstart = 25)$tot.withinss)
plot(1:10, wss_pca, type = "b", pch = 19, frame = FALSE,
xlab = "Number of clusters K", ylab = "Total within-clusters sum of squares",
main = "Elbow Method for Optimal Clusters after PCA")
abline(v = which.min(diff(diff(wss_pca))), lty = 2)
# Gap statistics after PCA
set.seed(123)
gap_stat_pca <- clusGap(pca_data, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
plot(gap_stat_pca, main = "Gap Statistics for Optimal Clusters after PCA")
k_optimal_gap_pca <- maxSE(gap_stat_pca$Tab[, "gap"], gap_stat_pca$Tab[, "SE.sim"])
# Silhouette analysis after PCA
sil_width_pca <- numeric(10)
for (k in 2:10) {
km.res <- kmeans(pca_data, centers = k, nstart = 25)
ss <- silhouette(km.res$cluster, dist(pca_data))
sil_width_pca[k] <- mean(ss[, 3])
}
plot(1:10, sil_width_pca, type = "b", pch = 19, frame = FALSE,
xlab = "Number of clusters K", ylab = "Average silhouette width",
main = "Silhouette Method for Optimal Clusters after PCA")
k_optimal_sil_pca <- which.max(sil_width_pca)
# Determine the optimal k after PCA using majority vote
k_optimal_pca <- as.numeric(names(sort(table(c(nbclust_results_euclidean_pca$Best.nc[1,], k_optimal_gap_pca, k_optimal_sil_pca)), decreasing = TRUE)[1]))
set.seed(123)
kmeans_pca_result <- kmeans(pca_data, centers = k_optimal_pca, nstart = 25)
# Display the clustering result
print(kmeans_pca_result)
# K-means clustering result plot after PCA
clusplot(pca_data, kmeans_pca_result$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
# BSS/TSS ratio after PCA
bss_pca <- kmeans_pca_result$betweenss
tss_pca <- kmeans_pca_result$totss
bss_tss_ratio_pca <- bss_pca / tss_pca
print(bss_tss_ratio_pca)
# Silhouette plot after PCA
sil_pca <- silhouette(kmeans_pca_result$cluster, dist(pca_data))
plot(sil_pca, border = NA)
avg_sil_width_pca <- mean(sil_pca[, 'sil_width'])
print(avg_sil_width_pca)
ch_index <- intCriteria(as.matrix(pca_data), kmeans_pca_result$cluster, "Calinski_Harabasz")
print(ch_index)
# Calculate Calinski-Harabasz Index for different k values
k_values <- 2:10
ch_indices <- sapply(k_values, function(k) {
km.res <- kmeans(pca_data, centers = k, nstart = 25)
ch_index <- intCriteria(as.matrix(pca_data), km.res$cluster, "Calinski_Harabasz")
return(ch_index$calinski_harabasz)
})
# Plot the Calinski-Harabasz Index values for different k
plot(k_values, ch_indices, type = "b", pch = 19, col = "blue",
xlab = "Number of Clusters (k)", ylab = "Calinski-Harabasz Index",
main = "Calinski-Harabasz Index for Different Numbers of Clusters")
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(readxl)
library(neuralnet)
library(Metrics)
library(dplyr)
library(DiagrammeR)
# Reading the Excel file
file_path <- "ExchangeUSD.xlsx"
exchange_data <- read_excel(file_path)
# Extracting the exchange rates
exchange_rates <- exchange_data[[3]]
# Creating time-delayed input-output matrices
time_delayed_matrix <- data.frame(
input_4 = lag(exchange_rates, 4),
input_3 = lag(exchange_rates, 3),
input_2 = lag(exchange_rates, 2),
input_1 = lag(exchange_rates, 1),
output = exchange_rates
)
# Removing rows with NA values
cleaned_matrix <- na.omit(time_delayed_matrix)
# Normalizing data
normalized_matrix <- as.data.frame(scale(cleaned_matrix))
# Function for min-max normalization
min_max_normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# Apply normalization to input and output columns
normalized_matrix <- as.data.frame(lapply(normalized_matrix, min_max_normalize))
# Splitting data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(normalized_matrix), size = 0.8 * nrow(normalized_matrix))
train_data <- normalized_matrix[train_indices, ]
test_data <- normalized_matrix[-train_indices, ]
# Define min and max for un-normalizing
min_output <- min(cleaned_matrix$output)
max_output <- max(cleaned_matrix$output)
# Function for un-normalizing
unnormalize <- function(x, min, max) {
(max - min) * x + min
}
# Function to evaluate model and display metrics
evaluate_model <- function(model, test_data, min_output, max_output) {
predictions <- predict(model, test_data[, -5])
predictions_unnorm <- unnormalize(predictions, min_output, max_output)
actual_unnorm <- unnormalize(test_data$output, min_output, max_output)
rmse <- sqrt(mean((actual_unnorm - predictions_unnorm)^2))
mae <- mean(abs(actual_unnorm - predictions_unnorm))
mape <- mean(abs(actual_unnorm - predictions_unnorm) / actual_unnorm) * 100
smape <- mean(200 * abs(actual_unnorm - predictions_unnorm) / (abs(actual_unnorm) + abs(predictions_unnorm)))
cat(sprintf("Testing Metrics:\nRMSE: %.5f\nMAE: %.5f\nMAPE: %.5f%%\nsMAPE: %.5f%%\n", rmse, mae, mape, smape))
return(rmse)
}
# Training and evaluating multiple models
set.seed(123)
neural_models <- list(
model_1 = neuralnet(output ~ input_4 + input_3 + input_2 + input_1, hidden = c(10, 7), data = train_data, linear.output = TRUE),
model_2 = neuralnet(output ~ input_3 + input_2 + input_1, hidden = c(4, 5), data = train_data, linear.output = TRUE),
model_3 = neuralnet(output ~ input_2 + input_1, hidden = 5, data = train_data, linear.output = TRUE),
model_4 = neuralnet(output ~ input_1, hidden = c(5, 5), data = train_data, linear.output = TRUE),
model_5 = neuralnet(output ~ input_2, hidden = 7, data = train_data, linear.output = TRUE),
model_6 = neuralnet(output ~ input_3, hidden = c(3, 6), data = train_data, linear.output = TRUE),
model_7 = neuralnet(output ~ input_4, hidden = 4, data = train_data, linear.output = TRUE),
model_8 = neuralnet(output ~ input_1 + input_2, hidden = c(8, 10), data = train_data, linear.output = TRUE),
model_9 = neuralnet(output ~ input_1 + input_3, hidden = 9, data = train_data, linear.output = TRUE),
model_10 = neuralnet(output ~ input_1 + input_4, hidden = c(3, 4), data = train_data, linear.output = TRUE),
model_11 = neuralnet(output ~ input_2 + input_3, hidden = 6, data = train_data, linear.output = TRUE),
model_12 = neuralnet(output ~ input_2 + input_4, hidden = c(2, 8), data = train_data, linear.output = TRUE),
model_13 = neuralnet(output ~ input_2 + input_4, hidden = 10, data = train_data, linear.output = TRUE),
model_14 = neuralnet(output ~ input_3 + input_4, hidden = 3, data = train_data, linear.output = TRUE),
model_15 = neuralnet(output ~ input_2 + input_3 + input_4, hidden = c(4, 8), data = train_data, linear.output = TRUE)
)
# Evaluate each model on testing data and store the RMSE
model_performance <- data.frame(Model = character(), RMSE = numeric(), Layers = integer())
for (model_name in names(neural_models)) {
cat(paste("Model:", model_name, "\n"))
model <- neural_models[[model_name]]
rmse <- evaluate_model(model, test_data, min_output, max_output)
layers <- length(model$weights[[1]]) - 1  # Subtract 1 to get the number of hidden layers
model_performance <- rbind(model_performance, data.frame(Model = model_name, RMSE = rmse, Layers = layers))
cat(sprintf("Testing Metrics:\nRMSE: %.5f\n\n", rmse))
}
# Find the best models
best_model_name <- model_performance[which.min(model_performance$RMSE), "Model"]
best_one_layer_model_name <- model_performance %>% filter(Layers == 1) %>% arrange(RMSE) %>% slice(1) %>% pull(Model)
best_two_layer_model_name <- model_performance %>% filter(Layers == 2) %>% arrange(RMSE) %>% slice(1) %>% pull(Model)
best_model <- neural_models[[best_model_name]]
best_one_layer_model <- neural_models[[best_one_layer_model_name]]
best_two_layer_model <- neural_models[[best_two_layer_model_name]]
# Function to visualize the neural network structure
plot_nn_structure <- function(nn_model, title) {
# Extract weights
weights <- nn_model$weights[[1]]
num_layers <- length(weights)
layers <- list()
# Create node labels and connections
nodes <- c("input_1", "input_2", "input_3", "input_4")
edges <- list()
for (i in 1:num_layers) {
layer_nodes <- paste0("H", i, "_", 1:nrow(weights[[i]]))
layers[[i]] <- layer_nodes
nodes <- c(nodes, layer_nodes)
if (i == 1) {
# Connect input layer to first hidden layer
for (input in 1:4) {
for (node in layer_nodes) {
edges <- c(edges, paste0("input_", input, " -> ", node))
}
}
} else {
# Connect hidden layers
for (node_from in layers[[i - 1]]) {
for (node_to in layer_nodes) {
edges <- c(edges, paste0(node_from, " -> ", node_to))
}
}
}
}
# Connect last hidden layer to output
for (node in layers[[num_layers]]) {
edges <- c(edges, paste0(node, " -> output"))
}
# Create graph
grViz(paste0("
digraph G {
graph [layout = dot]
node [shape = circle, style = filled, color = lightgreen]
edge [color = darkblue]
", paste(nodes, collapse = "; "), ";
output [shape = doublecircle, color = lightcoral];
", paste(edges, collapse = "; "), "
}
"))
}
# Visualize all models
for (i in 1:length(neural_models)) {
model_name <- paste("model", i, sep = "_")
cat("\nVisualizing Model:", model_name, "\n")
plot_nn_structure(neural_models[[i]], model_name)
}
# Visualize the best models
cat("\nVisualizing Best Models:\n")
plot_nn_structure(best_model, paste("Best Model Structure:", best_model_name))
plot_nn_structure(best_one_layer_model, paste("Best One Hidden Layer Model Structure:", best_one_layer_model_name))
plot_nn_structure(best_two_layer_model, paste("Best Two Hidden Layer Model Structure:", best_two_layer_model_name))
# Function to display metrics for a given model
display_metrics <- function(model, test_data, min_output, max_output) {
predictions <- predict(model, test_data[, -5])
predictions_unnorm <- unnormalize(predictions, min_output, max_output)
actual_unnorm <- unnormalize(test_data$output, min_output, max_output)
rmse <- sqrt(mean((actual_unnorm - predictions_unnorm)^2))
mae <- mean(abs(actual_unnorm - predictions_unnorm))
mape <- mean(abs(actual_unnorm - predictions_unnorm) / actual_unnorm) * 100
smape <- mean(200 * abs(actual_unnorm - predictions_unnorm) / (abs(actual_unnorm) + abs(predictions_unnorm)))
cat(sprintf("Testing Metrics:\nRMSE: %.5f\nMAE: %.5f\nMAPE: %.5f%%\nsMAPE: %.5f%%\n", rmse, mae, mape, smape))
}
# Display metrics for the best models
cat("\nBest Overall Model Metrics:\n")
display_metrics(best_model, test_data, min_output, max_output)
cat("\nBest One Hidden Layer Model Metrics:\n")
display_metrics(best_one_layer_model, test_data, min_output, max_output)
cat("\nBest Two Hidden Layer Model Metrics:\n")
display_metrics(best_two_layer_model, test_data, min_output, max_output)
# Graphical representation of results (Scatter plot) for the best overall model
best_model_predictions <- predict(best_model, test_data[, -5])
best_model_predictions_unnorm <- unnormalize(best_model_predictions, min_output, max_output)
test_actual_unnorm <- unnormalize(test_data$output, min_output, max_output)
plot(test_actual_unnorm, best_model_predictions_unnorm, col = 'darkgreen', main = 'Actual vs. Predicted Exchange Rates (Best Model)', pch = 18, cex = 0.7)
abline(a = 0, b = 1, col = "darkred", lwd = 2)  # Adding a line y=x for reference
# Graphical representation of results (Scatter plot) for the best one hidden layer model
one_layer_predictions <- predict(best_one_layer_model, test_data[, -5])
one_layer_predictions_unnorm <- unnormalize(one_layer_predictions, min_output, max_output)
plot(test_actual_unnorm, one_layer_predictions_unnorm, col = 'darkblue', main = 'Actual vs. Predicted Exchange Rates (Best One Hidden Layer Model)', pch = 18, cex = 0.7)
abline(a = 0, b = 1, col = "darkred", lwd = 2)  # Adding a line y=x for reference
# Graphical representation of results (Scatter plot) for the best two hidden layer model
two_layer_predictions <- predict(best_two_layer_model, test_data[, -5])
two_layer_predictions_unnorm <- unnormalize(two_layer_predictions, min_output, max_output)
plot(test_actual_unnorm, two_layer_predictions_unnorm, col = 'darkmagenta', main = 'Actual vs. Predicted Exchange Rates (Best Two Hidden Layer Model)', pch = 18, cex = 0.7)
abline(a = 0, b = 1, col = "darkred", lwd = 2)  # Adding a line y=x for reference
